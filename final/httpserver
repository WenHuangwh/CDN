#!/usr/bin/env python3
import argparse
import gzip
import http.client
import os
import pickle
import requests
import socket
import subprocess
import threading
import logging
import json

CACHE_FILENAME = "cache"
WEBSITE_LIST = "websites.txt"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class LatencyServer:
    """
    The LatencyServer uses scamper to test latency between the HTTP server and the client and sends the result back to the DNS server.
    """
    def __init__(self, port, scamper_port):
        """
        Initialize the LatencyServer.

        Parameters
        ----------
        port : int
            The port number to bind the LatencyServer.
        """
        self.port = port
        self.scamper_port = scamper_port

    def start(self):
        """
        Start the LatencyServer and handle incoming latency requests.
        """
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", self.port))
        sock.listen()

        while True:
            conn, addr = sock.accept()
            threading.Thread(target=self.handle_latency_request, args=(conn, addr)).start()

    def measure_latency_to_client(self, client_ip, latency = 999999):
        """
        Measure latency to a client using the given IP address.

        Parameters
        ----------
        client_ip : str
            The IP address of the client.

        Returns
        -------
        float
            The latency value to the client.
        """
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as scamper_sock:
            # Connect to the scamper control socket
            scamper_sock.connect(("localhost", self.scamper_port))

            # Attach to the control socket and set the data format to JSON
            scamper_sock.sendall(b"attach format json\n")

            # Send a ping command to scamper
            scamper_sock.sendall(f"ping -c 1 {client_ip}\n".encode())

            # Receive the result from scamper
            data = b""
            data = scamper_sock.recv(4096)
            data = scamper_sock.recv(4096)

            try:
                # Parse the JSON output
                output = data.decode()
                data_parts = output.split('\n')
                json_data = data_parts[1]
                parsed_data = json.loads(json_data)
                latency = parsed_data['statistics']['avg']
            except:
                latency = 999999
        # write_log(str(latency))
        return latency


    def handle_latency_request(self, conn, addr):
        """
        Handle the incoming latency request and send the latency back to the client.

        Parameters
        ----------
        conn : socket.socket
            The connection object for the client.
        addr : tuple
            The address of the client.
        """
        try:
            client_ip = conn.recv(1024).decode(errors='strict')
        except UnicodeDecodeError:
            logging.info(f"Error: Invalid UTF-8 received from {addr}")
            return
        latency = self.measure_latency_to_client(client_ip)
        conn.sendall(str(latency).encode())


class HTTPServer:
    """
    The HTTPServer class represents an HTTP server responsible for handling client requests, serving cached content, and fetching content from the origin server.
    """
    def __init__(self, port, origin):
        """
        Initialize the HTTPServer.

        Parameters
        ----------
        port : int
            The port number to bind the HTTPServer.
        origin : str
            The origin server for the CDN.
        """
        self.port = port
        self.origin = origin
        self.cache_manager = CacheManager(origin)

    def start(self):
        """
        Start the HTTPServer and handle incoming HTTP requests.
        """
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", self.port))
        sock.listen()

        while True:
            conn, addr = sock.accept()
            data = conn.recv(1024)
            response = self.handle_request(data.decode())
            conn.sendall(response)
            conn.close()

    def handle_request(self, request):
        """
        Handle the incoming HTTP request and generate the appropriate response.

        Parameters
        ----------
        request : str
            The incoming HTTP request.

        Returns
        -------
        bytes
            The response for the HTTP request.
        """
        request_parts = request.split()
        if len(request_parts) < 2:
            return b"HTTP/1.1 400 Bad Request\r\n\r\n"

        path = request_parts[1]

        if path == "/grading/beacon":
            return b"HTTP/1.1 204 No Content\r\n\r\n"
        # Have records in cache
        elif self.cache_manager.is_in_cache(path):
            headers, content = self.cache_manager.get_from_cache(path)
            response_bytes = bytearray()
            response_line = "HTTP/1.1 200 OK\r\n"
            response_bytes.extend(response_line.encode())

            for header, value in headers.items():
                header_line = f"{header}: {value}\r\n"
                response_bytes.extend(header_line.encode())

            response_bytes.extend(b"\r\n")
            response_bytes.extend(content)

            return bytes(response_bytes)
        # otherwise retrieve from origin
        # for invalid path, origin will respond with a 404
        else:
            conn = http.client.HTTPConnection(self.origin)
            # Add the Host header to the request
            headers = {"Host": self.origin}
            conn.request("GET", path, headers=headers)

            response = conn.getresponse()

            response_headers = response.getheaders()
            response_content = response.read()

            response_bytes = bytearray()
            response_line = f"HTTP/1.1 {response.status} {response.reason}\r\n"
            response_bytes.extend(response_line.encode())

            for header, value in response_headers:
                header_line = f"{header}: {value}\r\n"
                response_bytes.extend(header_line.encode())

            response_bytes.extend(b"\r\n")
            response_bytes.extend(response_content)

            return bytes(response_bytes)


class CacheManager:
    """
    The CacheManager class is responsible for managing the cache for the HTTP server. It stores the cached content, checks if a path is in the cache, and retrieves content from the cache.
    """
    def __init__(self, origin, max_cache_size=20 * 1024 * 1024):
        """
        Initialize the CacheManager.

        Parameters
        ----------
        origin : str
            The origin server for the CDN.
        max_cache_size : int, optional
            The maximum size of the cache in bytes. Default is 20 MB.
        """
        self.origin = origin
        self.max_cache_size = max_cache_size
        self.cache = self.setup_cache()

    def setup_cache(self):
        """
        Set up the cache by loading from the file or preparing a new cache.

        Returns
        -------
        dict
            The cache dictionary with cached content.
        """
        try:
            # Attempt to load the cache from the file
            with gzip.open(CACHE_FILENAME, "rb") as cache_file:
                cache = pickle.load(cache_file)
        except (FileNotFoundError, pickle.UnpicklingError):
            # If the file is not found or an error occurs, prepare the cache
            cache = self.prepare_cache()
            # Save the cache to a file named 'cache'
            with gzip.open(CACHE_FILENAME, "wb") as cache_file:
                pickle.dump(cache, cache_file)

        return cache

    def prepare_cache(self):
        """
        Prepare the cache by downloading the content from the origin server.

        Returns
        -------
        dict
            The cache dictionary with cached content.
        """
        # Read the content of websites.txt
        with open(WEBSITE_LIST, "r") as file:
            data = file.read()

        # Split the data into lines
        lines = data.splitlines()

        # Split each line by tabs and create a list of lists
        table = [line.split('\t') for line in lines]

        # Modify the article column and create a new list
        modified_table = [[row[0], row[1], row[2].replace(' ', '_')] for row in table]

        cache = {}
        cache_size = 0

        for i in range(1, len(modified_table)):
            rank, views, article = modified_table[i]
            url = f"http://{self.origin}/{article}"

            response = requests.get(url)

            if response.status_code == 200:
                content = response.content
                content_size = len(content)

                if cache_size + content_size <= self.max_cache_size:
                    cache[f"/{article}"] = content
                    cache_size += content_size
                else:
                    break
        return cache

    def is_in_cache(self, path):
        """
        Check if the given path is in the cache.

        Parameters
        ----------
        path : str
            The path to check in the cache.

        Returns
        -------
        bool
            True if the path is in the cache, False otherwise.
        """
        return path in self.cache

    def get_from_cache(self, path):
        """
        Get the content and headers for the given path from the cache.

        Parameters
        ----------
        path : str
            The path to get the content and headers from the cache.

        Returns
        -------
        tuple
            A tuple containing the headers and content for the given path.
        """
        content = self.cache[path]
        headers = {
            "Content-Type": "text/html; charset=utf-8",
            "Content-Length": len(content),
            "Cache-Control": "public, max-age=31536000",
        }
        return headers, content


# def write_log(log_entry):
#     with open("log.txt", "a") as log_file:
#         log_file.write(log_entry)
#         log_file.write("\n")


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--port', required=True, help="Port number to bind the HTTP server")
    parser.add_argument('-o', '--origin', required=True, help="Origin server for the CDN")
    args = parser.parse_args()

    scamper_port = int(args.port) + 1
    latency_port = int(args.port) + 2

    try:
        # Start the latency server
        latency_server = LatencyServer(latency_port, scamper_port)
        threading.Thread(target=latency_server.start).start()

        # Start the HTTP server
        http_server = HTTPServer(int(args.port), args.origin)
        logging.info(f"Starting HTTP server on port {args.port}")
        http_server.start()
    except Exception as e:
        logging.error(f"Error occurred: {e}")


if __name__ == "__main__":
    main()
