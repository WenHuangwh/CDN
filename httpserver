#!/usr/bin/env python3
import argparse
import gzip
import http.client
import os
import pickle
import requests
import socket
import subprocess
import sys
import threading
import time
import logging

CACHE_FILENAME = "cache"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class LatencyServer:

    def __init__(self, port):
        self.port = port

    # Start the LatencyServer, listen for incoming connections and handle them in separate threads
    def start(self):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", self.port))
        sock.listen()

        while True:
            conn, addr = sock.accept()
            threading.Thread(target=self.handle_latency_request, args=(conn, addr)).start()

    # Measure latency to the client using the Scamper tool and return the latency value
    def measure_latency_to_client(self, client_ip):
        result = subprocess.run(
            ["scamper", "-c", f"ping -c 1", "-i", client_ip],
            stdout=subprocess.PIPE,
            text=True
        )
        write_log("measure latency")
        write_log(result.stdout)
        lines = result.stdout.splitlines()
        line = lines[-1]
        latency = 0
        try: 
            latency = float(line.split()[3].split('/')[1]) # Extract the latency value
        except: 
            # a place holder value
            latency = 9999  
        return latency

    # Handle incoming latency requests, measure latency to the client, 
    # and send the latency value back
    def handle_latency_request(self, conn, addr):
        client_ip = conn.recv(1024).decode()
        latency = self.measure_latency_to_client(client_ip)
        write_log("latency value: ")
        write_log(str(latency))
        conn.sendall(str(latency).encode())


class HTTPServer:

    def __init__(self, port, origin):
        self.port = port
        self.origin = origin
        self.cache_manager = CacheManager(origin)

    # Start the HTTP server, listen for incoming connections, and handle the requests
    def start(self):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", self.port))
        sock.listen()

        print("HTTP server starts to listening")

        while True:
            conn, addr = sock.accept()
            data = conn.recv(1024)
            response = self.handle_request(data.decode())
            conn.sendall(response)
            conn.close()

    # Handle incoming HTTP requests and return the appropriate response based on the requested path
    def handle_request(self, request):
        request_parts = request.split()
        if len(request_parts) < 2:
            return b"HTTP/1.1 400 Bad Request\r\n\r\n"

        path = request_parts[1]

        if path == "/grading/beacon":
            return b"HTTP/1.1 204 No Content\r\n\r\n"
        # Have records in cache
        elif self.cache_manager.is_in_cache(path):
            write_log(f"Getting from cache: {path}")
            return self.cache_manager.get_from_cache(path)
        # otherwise retrieve from origin
        # for invalid path, origin will respond with a 404
        else:
            write_log(f"Getting from origin: {path}")
            conn = http.client.HTTPConnection(self.origin)
            # Start the timer
            start_time = time.time()
            # Add the Host header to the request
            headers = {"Host": self.origin}
            conn.request("GET", path, headers=headers)

            response = conn.getresponse()

            # End the timer and calculate the time taken
            end_time = time.time()
            time_taken = end_time - start_time

            response_headers = response.getheaders()
            response_content = response.read()

            response_bytes = bytearray()
            response_line = f"HTTP/1.1 {response.status} {response.reason}\r\n"
            response_bytes.extend(response_line.encode())

            for header, value in response_headers:
                header_line = f"{header}: {value}\r\n"
                response_bytes.extend(header_line.encode())

            response_bytes.extend(b"\r\n")
            response_bytes.extend(response_content)

            # Log the time taken to get the website from the origin server
            write_log(f"Time taken to get {path} from origin: {time_taken:.4f} seconds")

            return bytes(response_bytes)


class CacheManager:

    def __init__(self, origin, max_cache_size=20 * 1024 * 1024):
        self.origin = origin
        self.max_cache_size = max_cache_size
        self.cache = self.setup_cache()

    def setup_cache(self):
        try:
            # Attempt to load the cache from the file
            with gzip.open(CACHE_FILENAME, "rb") as cache_file:
                cache = pickle.load(cache_file)
        except (FileNotFoundError, pickle.UnpicklingError):
            # If the file is not found or an error occurs, prepare the cache
            cache = self.prepare_cache()
            # Save the cache to a file named 'cache'
            with gzip.open(CACHE_FILENAME, "wb") as cache_file:
                pickle.dump(cache, cache_file)

        return cache

    def prepare_cache(self):
        # Read the content of websites.txt
        with open("websites.txt", "r") as file:
            data = file.read()

        # Split the data into lines
        lines = data.splitlines()

        # Split each line by tabs and create a list of lists
        table = [line.split('\t') for line in lines]

        # Modify the article column and create a new list
        modified_table = [[row[0], row[1], row[2].replace(' ', '_')] for row in table]

        cache = {}
        cache_size = 0

        for i in range(1, len(modified_table)):
            rank, views, article = modified_table[i]
            url = f"http://{self.origin}/{article}"

            response = requests.get(url)

            if response.status_code == 200:
                content = response.content
                content_size = len(content)

                if cache_size + content_size <= self.max_cache_size:
                    cache[f"/{article}"] = content
                    cache_size += content_size
                else:
                    break
        return cache

    def is_in_cache(self, path):
        return path in self.cache

    def get_from_cache(self, path):
        return self.cache[path]


def write_log(log_entry):
    with open("log.txt", "a") as log_file:
        log_file.write(log_entry + "\n")


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--port', required=True, help="Port number to bind the HTTP server")
    parser.add_argument('-o', '--origin', required=True, help="Origin server for the CDN")
    args = parser.parse_args()

    latency_port = 20022

    try:
        # Start the latency server
        latency_server = LatencyServer(latency_port)
        threading.Thread(target=latency_server.start).start()

        # Start the HTTP server
        http_server = HTTPServer(int(args.port), args.origin)
        logging.info(f"Starting HTTP server on port {args.port}")
        http_server.start()
    except Exception as e:
        logging.error(f"Error occurred: {e}")


if __name__ == "__main__":
    main()

