#!/usr/bin/env python3
import http.client
import requests
import argparse
import socket
import os
import time
import signal
import pickle
import threading
import sys
from collections import OrderedDict
from functools import reduce
import gzip
import subprocess

cache_filename = "cache"

def measure_latency_to_client(client_ip):
    result = subprocess.run(
        ["scamper", "-c", f"ping -c 1", "-i", client_ip],
        stdout=subprocess.PIPE,
        text=True
    )
    write_log("measure latenct")
    write_log(result.stdout)
    lines = result.stdout.splitlines()
    for line in lines:
        write_log(line)
        if line.startswith("icmp_seq=0"):
            latency = float(line.split()[-2])
            return latency
    return float('inf')

def handle_latency_request(conn, addr):
    client_ip = conn.recv(1024).decode()
    latency = measure_latency_to_client(client_ip)
    conn.sendall(str(latency).encode())

def setupCache(origin, max_cache_size=20 * 1024 * 1024):
    cache = None
    try:
        # Attempt to load the cache from the file
        with gzip.open(cache_filename, "rb") as cache_file:
            cache = pickle.load(cache_file)
    except (FileNotFoundError, pickle.UnpicklingError):
        # If the file is not found or an error occurs, prepare the cache
        cache = prepare_cache(origin, max_cache_size)
        # Save the cache to a file named 'cache'
        with gzip.open(cache_filename, "wb") as cache_file:
            pickle.dump(cache, cache_file)

    return cache


def prepare_cache(origin, max_cache_size=20 * 1024 * 1024):
    # Read the content of websites.txt
    with open("websites.txt", "r") as file:
        data = file.read()

    # Split the data into lines
    lines = data.splitlines()

    # Split each line by tabs and create a list of lists
    table = [line.split('\t') for line in lines]

    # Modify the article column and create a new list
    modified_table = [[row[0], row[1], row[2].replace(' ', '_')] for row in table]

    cache = {}
    cache_size = 0

    for i in range(1, len(modified_table)):
        rank, views, article = modified_table[i]
        url = f"http://{origin}/{article}"

        response = requests.get(url)

        if response.status_code == 200:
            content = response.content
            content_size = len(content)

            if cache_size + content_size <= max_cache_size:
                cache[f"/{article}"] = content
                cache_size += content_size
            else:
                break
    return cache

# Function to write log entries to a file
def write_log(log_entry):
    with open("log.txt", "a") as log_file:
        log_file.write(log_entry + "\n")


# Function to handle incoming HTTP requests and return the appropriate response
def handle_request(request, origin, cache):
    request_parts = request.split()
    if len(request_parts) < 2:
        return b"HTTP/1.1 400 Bad Request\r\n\r\n"
    
    path = request_parts[1]

    if path == "/grading/beacon":
        return b"HTTP/1.1 204 No Content\r\n\r\n"
    # Have records in cache
    elif path in cache:
        write_log(f"Getting from cache: {path}")
        return cache[path]
    # otherwise retrieve from origin
    # for invalid path, origin will respond with a 404
    else:
        write_log(f"Getting from origin: {path}")
        conn = http.client.HTTPConnection(origin)
        # Start the timer
        start_time = time.time()
        # Add the Host header to the request
        headers = {"Host": origin}
        conn.request("GET", path, headers=headers)
        
        response = conn.getresponse()
        
        # End the timer and calculate the time taken
        end_time = time.time()
        time_taken = end_time - start_time

        response_headers = response.getheaders()
        response_content = response.read()

        response_bytes = bytearray()
        response_line = f"HTTP/1.1 {response.status} {response.reason}\r\n"
        response_bytes.extend(response_line.encode())

        for header, value in response_headers:
            header_line = f"{header}: {value}\r\n"
            response_bytes.extend(header_line.encode())

        response_bytes.extend(b"\r\n")
        response_bytes.extend(response_content)

        # Log the time taken to get the website from the origin server
        write_log(f"Time taken to get {path} from origin: {time_taken:.4f} seconds")
        
        return bytes(response_bytes)

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--port', required=True, help="Port number to bind the HTTP server")
    parser.add_argument('-o', '--origin', required=True, help="Origin server for the CDN")
    args = parser.parse_args()

    latency_port = 20022
    # Set up a socket for latency measurement requests from DNS server
    latency_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    latency_sock.bind(("", latency_port))
    latency_sock.listen()

    # Start a new thread to handle latency measurement requests
    threading.Thread(target=lambda: handle_latency_request(*latency_sock.accept())).start()

    # Set up the socket for the HTTP server
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(("", int(args.port)))
    sock.listen()

    print("http server starts to listening")

    cache = setupCache(args.origin)

    while True:
        conn, addr = sock.accept()
        data = conn.recv(1024)
        response = handle_request(data.decode(), args.origin, cache)
        conn.sendall(response)
        conn.close()

if __name__ == "__main__":
    main()
